# -*- coding: utf-8 -*-
"""KHUSHI_SNA_FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1reTX--DGGZtyO4UVVGQxT72hdJOGHOVe

find cycles
"""

import csv

# read csv file
with open('/content/TED_Talk1.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    talks = {}
    
    for row in reader:
        talk_id = row['talk_id']
        related_talks = row['related_talks']
        if related_talks:  # check if string is not empty
            try:
                related_talks = eval(related_talks)
            except (SyntaxError, NameError):  # catch eval errors
                related_talks = []
        else:
            related_talks = []
        tags = row['talks_tags'].split(', ')  # read tags and split into a list
        talks[talk_id] = {'related_talks': related_talks, 'tags': tags}  # store related talks and tags

# identify cycles in the graph
cycles = []
cycle_lengths = []
max_cycles = 800  # set maximum number of cycles to find
visited = set()
for talk_id in talks:
    stack = [(talk_id, [])]
    while stack:
        try:
            talk, path = stack.pop()
            if talk in visited:
                if talk_id in path:
                    cycle = path[path.index(talk_id):] + [talk_id]
                    cycles.append(cycle)
                    cycle_lengths.append(len(cycle))  # store cycle length
                    if len(cycles) == max_cycles:
                        break
            else:
                visited.add(talk)
                for related_talk in talks[talk]['related_talks']:
                    stack.append((related_talk['id'], path + [talk]))
        except KeyError:
            pass

    if len(cycles) == max_cycles:
        break

 #print cycles with tags and lengths
for i, cycle in enumerate(cycles):
    cycle_length = cycle_lengths[i]
    print(f'Cycle {i+1} (length {cycle_length}):')
    for talk_id in cycle:
        tags = ', '.join(talks[talk_id]['tags'])  # get tags for the talk
        print(f'\t{talk_id}: {tags}')

# max and min cycles

#compute cycle lengths
cycle_lengths = [len(cycle) for cycle in cycles]

# find max and min cycles
max_cycle_idx = max(range(len(cycles)), key=lambda i: cycle_lengths[i])
min_cycle_idx = min(range(len(cycles)), key=lambda i: cycle_lengths[i])


# print cycle with min length
min_cycle = cycles[min_cycle_idx]
min_cycle_length = cycle_lengths[min_cycle_idx] - 1
print(f'\nCycle with min length (length {min_cycle_length}):')
for talk_id in min_cycle:
    tags = ', '.join(talks[talk_id]['tags'])  # get tags for the talk
    print(f'\t{talk_id}: {tags}')        

# print cycle with max length
max_cycle = cycles[max_cycle_idx]
max_cycle_length = cycle_lengths[max_cycle_idx] - 1
print(f'\nCycle with max length (length {max_cycle_length}):')
for talk_id in max_cycle:
    tags = ', '.join(talks[talk_id]['tags'])  # get tags for the talk
    print(f'\t{talk_id}: {tags}')

#to make the quality of length in terms if quantity using quartiles
import numpy as np

cycle_lengths = [len(cycle) for cycle in cycles]
q1, q2, q3 = np.percentile(cycle_lengths, [25, 50, 75])

short_cycles = [i+1 for i, cycle in enumerate(cycles) if len(cycle) <= q1]
medium_cycles = [i+1 for i, cycle in enumerate(cycles) if q1 < len(cycle) <= q3]
long_cycles = [i+1 for i, cycle in enumerate(cycles) if len(cycle) > q3]

# print cycle lists by length category
print("Short cycles:", short_cycles)
print("Medium cycles:", medium_cycles)
print("Long cycles:", long_cycles)

"""Analyze the distribution of cycle lengths using histogram or box plot to visualize the distribution. To identify which length categories are the most common or have the highest frequency of occurrence."""

# Analyze the distribution of cycle lengths using histogram or box plot to visualize the distribution. To identify which length categories are the most common or have the highest frequency of occurrence.
import matplotlib.pyplot as plt

# get cycle lengths
cycle_lengths = [len(cycle) for cycle in cycles]

# create box plot
plt.boxplot(cycle_lengths)

# set labels and title
plt.xlabel('Cycle length')
plt.title('Distribution of cycle lengths')

# show plot
plt.show()

import matplotlib.pyplot as plt

# create histogram
plt.hist(cycle_lengths, bins=20, color='#86bf91')

# add labels and title
plt.title('Distribution of Cycle Lengths')
plt.xlabel('Cycle Length')
plt.ylabel('Frequency')

# add a vertical line for each quartile
plt.axvline(q1, color='r', linestyle='--', label='Q1')
plt.axvline(q2, color='g', linestyle='--', label='Q2')
plt.axvline(q3, color='b', linestyle='--', label='Q3')

# add legend
plt.legend()

# display the plot
plt.show()

rec 5 playlists from a given cycle

# Asking the user for a cycle number and recommending 5 unique playlists from that cycle.

# prompt the user to input a cycle number
cycle_num = int(input("Enter cycle number (1-8000): "))

# ensure that the input is within the range of possible cycles
if cycle_num < 1 or cycle_num > 8000:
    print("Invalid cycle number. Please enter a number between 1 and 8000.")
else:
    # get the talks in the cycle
    cycle_talks = set(cycles[cycle_num-1])
    
    # create a set of all talks that have already been used in playlists
    used_talks = set()
    for playlist in playlists:
        used_talks.update(set(playlist['talks']))
    
    # create a list of unused talks in the cycle
    unused_talks = list(cycle_talks - used_talks)
    
    # create 5 unique playlists from the cycle
    recommended_playlists = []
    for i in range(5):
        if unused_talks:
            # if there are unused talks, choose a random selection of 10 talks
            playlist_talks = set(random.sample(unused_talks, min(10, len(unused_talks))))
            unused_talks = list(set(unused_talks) - playlist_talks)
        else:
            # if all talks have been used, choose a random selection of 10 talks from the cycle
            playlist_talks = set(random.sample(cycle_talks, min(10, len(cycle_talks))))
        # create a set of tags for the playlist
        playlist_tags = set()
        for talk_id in playlist_talks:
            playlist_tags.update(talks[talk_id]['tags'])
        # add the playlist to the recommended playlists
        recommended_playlists.append({'name': f'Playlist {i+1}', 'talks': list(playlist_talks), 'tags': list(playlist_tags)})
    
    # print the recommended playlists
    print(f"\nHere are 5 unique playlists recommended from cycle {cycle_num}:")
    for playlist in recommended_playlists:
        print(f"{playlist['name']}:")
        talk_count = 0
        for talk_id in playlist['talks']:
            tags = ', '.join(talks[talk_id]['tags'])
            print(f"\t{talk_id}: {tags}")
            talk_count += 1
            if talk_count == 10:
                break
        print()

"""check if we remove 1 talk from a cycle and then compare the tags of the leftover talks with that of the cycle, will it make any difference or not"""

import numpy as np

def calculate_accuracy(playlist):
    """Calculates recommendation accuracy for a given playlist"""
    # randomly select one talk to remove from the playlist
    talk_ids = list(map(int, playlist['talks']))  # convert talk IDs to integers
    talk_to_remove = np.random.choice(talk_ids)
    # create a set of tags for the remaining talks in the playlist
    playlist_tags = set(playlist['tags'])
    remaining_talks = [talk for talk in talk_ids if talk != talk_to_remove and talk in talk_ids]
    # calculate the accuracy by checking if the removed talk's tags are in the remaining talks' tags
    removed_talk = [talk for talk in playlist['talks'] if talk['id'] == str(talk_to_remove)][0]
    removed_talk_tags = set(removed_talk['tags'])
    remaining_talk_tags = set()
    for talk in playlist['talks']:
        if talk['id'] != str(talk_to_remove):
            remaining_talk_tags.update(set(talk['tags']))
    accuracy = len(removed_talk_tags.intersection(remaining_talk_tags)) / len(removed_talk_tags)
    return accuracy

# categorize cycles by length
short_cycles = []
medium_cycles = []
long_cycles = []
for cycle in cycles:
    cycle_length = len(cycle)
    if cycle_length <= 5:
        short_cycles.append(cycle)
    elif cycle_length <= 10:
        medium_cycles.append(cycle)
    else:
        long_cycles.append(cycle)

from scipy.stats import f_oneway

# compare recommendation accuracy between categories using ANOVA
fvalue, pvalue = f_oneway(short_cycle_accuracy, medium_cycle_accuracy, long_cycle_accuracy)
if pvalue < 0.05:
    print('There is a significant difference in recommendation accuracy between cycle length categories.')
else:
    print('There is no significant difference in recommendation accuracy between cycle length categories.')

"""making dag from the data"""

import csv
import networkx as nx
import matplotlib.pyplot as plt

# create an empty directed graph
G = nx.DiGraph()

with open('TED_Talk_TagsRelated_cols.csv', 'r', errors='ignore') as infile:
    reader = csv.reader(infile, dialect='excel')
    next(reader)
    
    count = 0
    for in_line in reader:
        if count == 200:
            break
        
        talk_tags = in_line[1].strip('[]').replace('\'', '').split(', ')
        G.add_node(in_line[0])
        for node in G.nodes():
            if node != in_line[0]:
                node_tags = nx.get_node_attributes(G, 'tags')[node]
                if set(node_tags).issubset(set(talk_tags)):
                    G.add_edge(node, in_line[0])
        nx.set_node_attributes(G, {in_line[0]: {'tags': talk_tags}})
        count += 1

# remove isolated nodes
isolated_nodes = list(nx.isolates(G))
G.remove_nodes_from(isolated_nodes)


# plot the graph
pos = nx.spring_layout(G)
nx.draw(G, pos, with_labels=True)
edge_labels = nx.get_edge_attributes(G, 'tags')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.show()

"""making the dag, calculate relevance score of a talk based on its PageRank score(measures the importance of a talk in the network) and edge weights(measure the strength of the relationship between two talks). Recommending related talks based on a given talk. 
Compute the relevance scores of the related talks, and return the top k related talks with the highest relevance scores.
The code provides an example usage of the recommend function to recommend talks related to the talk with ID '2072'.
"""

import csv
import networkx as nx
import heapq

# create an empty directed graph
G = nx.DiGraph()

# read the TED Talk tags and related talks from a CSV file
with open('TED_Talk_TagsRelated_cols.csv', 'r', errors='ignore') as infile:
    reader = csv.reader(infile, dialect='excel')
    next(reader)
    
    count = 0
    for in_line in reader:
        if count == 200:
            break
        
        talk_tags = in_line[1].strip('[]').replace('\'', '').split(', ')
        G.add_node(in_line[0])
        for node in G.nodes():
            if node != in_line[0]:
                node_tags = nx.get_node_attributes(G, 'tags')[node]
                if set(node_tags).issubset(set(talk_tags)):
                    G.add_edge(node, in_line[0], weight=len(set(node_tags).intersection(set(talk_tags))))
        nx.set_node_attributes(G, {in_line[0]: {'tags': talk_tags}})
        count += 1

# remove isolated nodes
isolated_nodes = list(nx.isolates(G))
G.remove_nodes_from(isolated_nodes)

# define a function to compute the relevance score of a talk based on its PageRank score and edge weights
def relevance_score(talk, damping_factor=0.85):
    pagerank_score = nx.pagerank(G, alpha=damping_factor, weight='weight')[talk]
    return pagerank_score * len(G.in_edges(talk))

# define a function to recommend related talks based on a given talk
def recommend(talk, top_k=10):
    related_talks = []
    for neighbor in G.neighbors(talk):
        score = relevance_score(neighbor)
        heapq.heappush(related_talks, (-score, neighbor))
    return [talk[1] for talk in heapq.nsmallest(top_k, related_talks)]

for node in G.nodes():
    recommendations = recommend(node)
    if recommendations:
        print(f"Recommendations for node {node}:")
        print(recommendations)

"""**see if the rec in the csv are near those recommended by the algo**"""

# function to calculate the Jaccard similarity between two sets
def jaccard_similarity(set1, set2):
    intersection = set1.intersection(set2)
    union = set1.union(set2)
    return len(intersection) / len(union)

# Load the recommendations from the dataset and the recommendations generated by the DAG into sets
with open('TED_Talk_TagsRelated_cols.csv', 'r') as infile:
    reader = csv.reader(infile)
    next(reader)

    dataset_recommendations = set()
    for in_line in reader:
        dataset_recommendations.add(in_line[1])

# calculate Jaccard similarity for each node in the DAG and store in a list
dag_similarities = []
for node in G.nodes():
    node_recommendations = set(recommend(node))
    if node_recommendations:
        node_similarity = jaccard_similarity(dataset_recommendations, node_recommendations)
        dag_similarities.append(node_similarity)

# Print the average Jaccard similarity across all nodes in the DAG
if dag_similarities:
    avg_similarity = sum(dag_similarities) / len(dag_similarities)
    print('Jaccard similarity between the given code and the recommendations in the column:', avg_similarity)
else:
    print('No nodes with non-empty recommendations found')

import nltk
nltk.download('punkt')

import pandas as pd
import ast
from nltk.tokenize import word_tokenize

# read the TED_Talk1.csv file and extract the related_talks column
df = pd.read_csv('TED_Talk1.csv')
related_talks = df['related_talks']

# tokenize the code and related_talks
code_tokens = set(word_tokenize(open('new1.py').read()))
related_talks_tokens = set(word_tokenize(str(related_talks)))

# calculate the Jaccard similarity between the two sets
jaccard_similarity = len(code_tokens.intersection(related_talks_tokens)) / len(code_tokens.union(related_talks_tokens))

print("Jaccard similarity between the given code and the recommendations in the 'related_talks' column:", jaccard_similarity)

import pandas as pd
import ast
from nltk.tokenize import word_tokenize

# read the TED_Talk1.csv file and extract the related_talks column
df = pd.read_csv('TED_Talk1.csv')
related_talks = df['related_talks']

# tokenize the code and related_talks
code_tokens = set(word_tokenize(open('interaction_matrix.py').read()))
related_talks_tokens = set()
for talks in related_talks:
    if isinstance(talks, str):
        talks_list = ast.literal_eval(talks)
        for talk in talks_list:
            if isinstance(talk, str):
                talk_tokens = set(word_tokenize(talk))
                related_talks_tokens = related_talks_tokens.union(talk_tokens)

# calculate the Jaccard similarity between the two sets
jaccard_similarity = len(code_tokens.intersection(related_talks_tokens)) / len(code_tokens.union(related_talks_tokens))

print("Jaccard similarity between the given code and the recommendations in the 'related_talks' column:", jaccard_similarity)